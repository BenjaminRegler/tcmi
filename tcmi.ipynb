{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<div id=\"teaser\" style=' background-position:  right center; background-size: 00px; background-repeat: no-repeat; \n",
    "    padding-top: 20px;\n",
    "    padding-right: 10px;\n",
    "    padding-bottom: 170px;\n",
    "    padding-left: 10px;\n",
    "    border-bottom: 14px double #333;\n",
    "    border-top: 14px double #333;' > \n",
    "\n",
    "   \n",
    "   <div style=\"text-align:center\">\n",
    "    <b><font size=\"6.4\">Total cumulative mutual information</font></b>    \n",
    "  </div>\n",
    "    \n",
    "<p>\n",
    " created by:\n",
    " Benjamin Regler<sup>1</sup>, \n",
    " Matthias Scheffler<sup>1</sup>,\n",
    " and Luca Ghiringhelli<sup> 1</sup> <br><br>\n",
    "<sup>1</sup> Fritz Haber Institute of the Max Planck Society, Faradayweg 4-6, D-14195 Berlin, Germany <br>\n",
    "<span class=\"nomad--last-updated\" data-version=\"v1.0.2\">[Last updated: January 20, 2020]</span>\n",
    "</p>\n",
    "  \n",
    "<div> \n",
    "    <img style=\"float: left;\" src=\"assets/tcmi/logo-mpg.png\" width=\"200\"> \n",
    "    <img style=\"float: right;\" src=\"assets/tcmi/logo-nomad.png\" width=\"250\">\n",
    "</div>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T16:58:13.885666Z",
     "start_time": "2020-01-13T16:58:13.881334Z"
    }
   },
   "source": [
    "Welcome to the supplementary material for the publication:\n",
    "\n",
    "<div style=\"padding: 1ex; margin-top: 1ex; margin-bottom: 1ex; border-style: dotted; border-width: 1pt; border-color: blue; border-radius: 3px;\">\n",
    "B. Regler, M. Scheffler, and L. M. Ghiringhelli: \"TCMI: a non-parametric mutual-dependence estimator for multivariate continuous distributions\" [<a href=\"https://arxiv.org/abs/2001.11212\">arxiv:2001.11212</a>] [<a href=\"https://arxiv.org/pdf/2001.11212\">pdf</a>]\n",
    "</div>\n",
    "\n",
    "This interactive notebook includes the original implementation of total cumulative mutual information (TCMI) to reproduce the main results presented in the publication.\n",
    "\n",
    "TCMI is a measure of the relevance of mutual dependencies based on cumulative probability distributions. TCMI can be estimated directly from sample data and is a non-parametric, robust and deterministic measure that facilitates comparisons and rankings between feature sets with different cardinality. The ranking induced by TCMI allows for feature selection, i.e. the identification of the set of relevant features that are statistical related to the process or the property of a system, while taking into account the number of data samples as well as the cardinality of the feature subsets.\n",
    "\n",
    "It is compared to [Cumulative mutual information (CMI)](https://dx.doi.org/10.1137/1.9781611972832.22), [Multivariate maximal correlation analysis (MAC)](http://proceedings.mlr.press/v32/nguyenc14.html), [Universal dependency analysis (UDS)](https://dx.doi.org/10.1137/1.9781611974348.89), and [Monte Carlo dependency estimation (MCDE)](https://dx.doi.org/10.1145/3335783.3335795).\n",
    "\n",
    "This repository (notebook and code) is released under the [Apache License, Version 2.0](http://www.apache.org/licenses/). Please see the [LICENSE](LICENSE) file.\n",
    "\n",
    "---\n",
    "\n",
    "**Important notes:**\n",
    "<ul style=\"color: #8b0000; font-style: italic;\">\n",
    "<li>All comparisons have been computed with the Java package <code>MCDE</code> written in Scala, which is not part of the repository. To use the most recent and maintained implementation, please visit <a href=\"https://github.com/edouardfouche/MCDE\">https://github.com/edouardfouche/MCDE</a> and run all examples with 50,000 iterations.</li>\n",
    "<li>For the sake of simplicity, all results have been cached. However, results can be recalculated after adjusting the respective test sections. Depending on the test, the calculation time ranges from minutes to days.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-06T20:03:16.271Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Toggle caching\n",
    "use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-06T20:03:16.273Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import joblib\n",
    "import warnings\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Our package\n",
    "import tcmi\n",
    "from tcmi import utils\n",
    "from tcmi import entropy\n",
    "from tcmi.cache import Cache\n",
    "from tcmi.subspace_search import get_subspaces\n",
    "from tcmi.estimators import DependenceEstimator\n",
    "\n",
    "\n",
    "def get_storage_data(key, default=None, overwrite=False):\n",
    "    \"\"\"Wrapper for storage access. Uses use_cache.\n",
    "    \"\"\"\n",
    "    return storage.get(key, default) if use_cache or overwrite else default\n",
    "\n",
    "\n",
    "# Main loop\n",
    "if __name__ == '__main__': \n",
    "    # Provide cache\n",
    "    storage = Cache('data/tcmi')\n",
    "    \n",
    "    # Configure plot environment\n",
    "    mpl.rc('font', family='sans', size=14)\n",
    "    mpl.rcParams.update({\n",
    "        'figure.facecolor': (0, 0, 0, 0),\n",
    "        'axes.facecolor': (0, 0, 0, 0),\n",
    "        'xtick.labelsize': 14,\n",
    "        'ytick.labelsize': 14\n",
    "    })\n",
    "    \n",
    "    # Define colors\n",
    "    cmap1 = plt.get_cmap('cividis')\n",
    "    cmap2 = plt.get_cmap('RdBu_r')\n",
    "    cmap_neutral = plt.get_cmap('binary')\n",
    "    \n",
    "    neutral_color1 = '#666666'\n",
    "    neutral_color2 = '#999999'\n",
    "    neutral_color3 = '#cccccc'\n",
    "    \n",
    "    # General settings (please do not touch)\n",
    "    kwargs = dict(n_jobs=-1, return_scores=True)\n",
    "    seed = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T17:02:13.549676Z",
     "start_time": "2020-01-13T17:02:13.547302Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## 1. Basic tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section studies some of the properties of total cumulative mutual information. In particular, we check that the\n",
    "\n",
    "- score is a monotonous function in the order of the conditionals\n",
    "- score attains it's maximum and minimun theoretical values (linear and zero case)\n",
    "- correction vanishes with increasing number of data samples\n",
    "- adjusted version of the score is (almost) constant with respect to subset dimensionality and sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:36.476498Z",
     "start_time": "2020-02-06T20:01:36.469713Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test case 1\n",
    "methods = ['cmi', 'mac', 'uds', 'mcde']\n",
    "size = 200\n",
    "\n",
    "# Test case 2\n",
    "sizes = [10, 50, 100, 500]\n",
    "n_repeats = 50\n",
    "dimensions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.1. Monotonicity check and ranking of monotonous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T09:24:46.755127Z",
     "start_time": "2020-01-14T09:24:46.752304Z"
    },
    "hidden": true
   },
   "source": [
    "**Test**: Monotonicity check of score<br />\n",
    "**Expected**: linear must be first, followed by step functions, zero must be last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:36.578237Z",
     "start_time": "2020-02-06T20:01:36.481310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=seed)\n",
    "tests = {\n",
    "    # Common operations\n",
    "    'linear': np.arange(size),\n",
    "    'exponential': np.exp(np.linspace(0, 1, num=size)),\n",
    "\n",
    "    # Adding copies of values\n",
    "    'step_2': np.repeat(np.arange(size // 2), 2),\n",
    "    'step_4': np.repeat(np.arange(size // 4), 4),\n",
    "    'step_8': np.repeat(np.arange(size // 8), 8),\n",
    "\n",
    "    # Interleave copies\n",
    "    'sawtooth_2': np.stack(tuple(zip(*[np.arange(2) for i in range(size // 2)])),\n",
    "                     axis=1).flatten(),\n",
    "    'sawtooth_4': np.stack(tuple(zip(*[np.arange(4) for i in range(size // 4)])),\n",
    "                     axis=1).flatten(),\n",
    "    'sawtooth_8': np.stack(tuple(zip(*[np.arange(8) for i in range(size // 8)])),\n",
    "                     axis=1).flatten(),\n",
    "    \n",
    "    'random': rng.random_sample(size),\n",
    "    'zero': np.zeros(size)\n",
    "}\n",
    "\n",
    "ranks = {}\n",
    "output = np.arange(size) + 1\n",
    "\n",
    "for name, value in tests.items():\n",
    "    # Compute scores from other dependency measures for comparison\n",
    "    key = 'monotonicity-check-' + name   \n",
    "    scores = get_storage_data(key, [], overwrite=True)\n",
    "    \n",
    "    if not scores:\n",
    "        score, scores = entropy.cumulative_mutual_information(output, (value, ), **kwargs)\n",
    "        scores = list(s.mean() for s in scores)\n",
    "        \n",
    "        # Compute scores from other dependency measures for comparison\n",
    "        for method in methods:\n",
    "            estimator = DependenceEstimator(method=method, n_jobs=-1)\n",
    "            score = estimator.score(value, output)\n",
    "            scores.append(score)\n",
    "    \n",
    "    ranks[name] = [np.around(s, decimals=4) for s in scores]\n",
    "    \n",
    "    # Compute correlation\n",
    "    correlation = 0\n",
    "    if np.unique(value).size > 1:\n",
    "        correlation = sp.stats.spearmanr(output, value)[0]\n",
    "    ranks[name].insert(0, np.around(correlation**2, decimals=4))\n",
    "\n",
    "# Show ranking\n",
    "ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)\n",
    "index, data = tuple(zip(*ranks))\n",
    "data = np.array([(rho, total, s, np.sum([s01, s02])) + tuple(rest)\n",
    "                 for rho, total, s, s01, s02, *rest in data])\n",
    "\n",
    "columns = ['rho2', 'adjusted_score', 'score', 'score0']\n",
    "columns.extend(methods)\n",
    "\n",
    "ranks = pd.DataFrame(data, index=index, columns=columns)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.2. Dependence of the correction term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Test**: Baseline correction dependence with respect to number of data samples<br />\n",
    "**Expected**: Baseline correction monotonically decreases in the number of data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:37.268567Z",
     "start_time": "2020-02-06T20:01:36.584168Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "key = 'correction-term'\n",
    "steps = np.concatenate((np.linspace(1, 9, num=9), np.linspace(10, 100, num=10)))\n",
    "\n",
    "values = get_storage_data(key, [])\n",
    "if len(values) == 0:\n",
    "    values = np.zeros(len(steps), dtype=np.float_)\n",
    "    for i, size in enumerate(steps):\n",
    "        output = np.arange(size) + 1\n",
    "        if size == 1:\n",
    "            values[i] = 1\n",
    "            continue\n",
    "\n",
    "        ce = entropy.cumulative_entropy(output)\n",
    "        hce0 = entropy.cumulative_baseline_correction(output, output, n_jobs=-1)\n",
    "        score0 = 1 - hce0 / ce\n",
    "\n",
    "        # Save average score\n",
    "        values[i] = score0.mean()\n",
    "    \n",
    "##\n",
    "# Plot dependence\n",
    "##    \n",
    "offset = 19\n",
    "fig, ax = plt.subplots(figsize=plt.figaspect(1), dpi=100)\n",
    "\n",
    "# Plot baseline correction\n",
    "ax.plot(steps[:offset], values[:offset], '-o', color=cmap1(0.1),\n",
    "        clip_on=False, linewidth=2, label=r'$\\langle \\hat{\\mathcal{D}}^{(\\prime)}_0(Y; X) \\rangle$')\n",
    "ax.plot(steps[:offset], steps[:offset]**(-2/3), color=cmap2(0.9),\n",
    "        linestyle='--', label='Fit')\n",
    "\n",
    "# Show approximate relationship\n",
    "ax.annotate(r'$\\langle \\hat{\\mathcal{D}}^{(\\prime)}_0(Y; X) \\rangle \\sim n^{-2/3}$',\n",
    "            (20, 0.4), color=cmap2(0.9),fontsize=15)\n",
    "\n",
    "# Plot styles\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.spines['left'].set_position(('outward', 10))\n",
    "ax.spines['bottom'].set_position(('outward', 10))\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(0, steps[offset - 1])\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "ax.set_xlabel('Sample size $n$')\n",
    "ax.set_ylabel(r'$\\langle \\hat{\\mathcal{D}}_0(Y; X) \\rangle\\ /\\ \\langle \\hat{\\mathcal{D}}_0^\\prime(Y; X) \\rangle$')\n",
    "\n",
    "ax.legend(loc='upper right', facecolor='w', frameon=False, bbox_to_anchor=(1.1, 1.05))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show table\n",
    "pd.DataFrame(np.atleast_2d(values), columns=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.3. Baseline correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Test**: Baseline correction of measure with respect to number of samples and dimension<br />\n",
    "**Expected**: Constant scores for different dimensionalities and number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:37.410841Z",
     "start_time": "2020-02-06T20:01:37.278460Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtypes = [('dimension', np.int), ('total_score', np.float_),\n",
    "          ('score', np.float_), ('score_corr', np.float_), ('score0', np.float_)]\n",
    "    \n",
    "results = {}\n",
    "for size in sizes:\n",
    "    output = np.arange(size) + 1\n",
    "    print('\\nSize: {:d}'.format(size))\n",
    "    \n",
    "    dtypes = [('dimension', np.int), ('total_score', np.float_),\n",
    "              ('score', np.float_), ('score_corr', np.float_), ('score0', np.float_)]\n",
    "    \n",
    "    scores_mean = np.zeros(dimensions + 1, dtype=dtypes)\n",
    "    scores_std = np.zeros(dimensions + 1, dtype=dtypes)\n",
    "    for i, dimension in enumerate(range(dimensions + 1)):\n",
    "        print('- Dimension: {:d} '.format(dimension), end='   ')\n",
    "        key = 'baseline_correction_size={:d}_dimension={:d}_repeats={:d}'.format(size, dimension, n_repeats)\n",
    "        \n",
    "        scores = get_storage_data(key, None)\n",
    "        if scores is None:            \n",
    "            # Make sure results are reproducible\n",
    "            rng = np.random.RandomState(seed=seed)\n",
    "            scores = []\n",
    "            \n",
    "            for _ in range(n_repeats):\n",
    "                if dimension < 1:\n",
    "                    # Add predictions on special first dimensional case\n",
    "                    z = [rng.random_sample(size)]\n",
    "                    \n",
    "                    noise = 2 * rng.random_sample(size) - 1\n",
    "                    z.append(z[0] + 0.1 * noise)\n",
    "                else:\n",
    "                    z = [rng.random_sample(size) for _ in range(dimension)]\n",
    "            \n",
    "                _, score = entropy.cumulative_mutual_information(output, z, **kwargs)\n",
    "                scores.append(score)\n",
    "            \n",
    "                # Show update\n",
    "                print('.', end='')\n",
    "            \n",
    "        # Collect statistics\n",
    "        scores_mean[i] = (dimension, ) + tuple(np.mean(score) for score in zip(*scores))\n",
    "        scores_std[i] = (dimension, ) + tuple(np.std(score) for score in zip(*scores))\n",
    "        print('')\n",
    "    results[size] = (scores_mean, scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:38.318289Z",
     "start_time": "2020-02-06T20:01:37.421003Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Ploat data\n",
    "##\n",
    "\n",
    "fig, axs = plt.subplots(1, len(results), figsize=plt.figaspect(1/(len(results) + 0.1)))\n",
    "axs = axs.flatten()\n",
    "\n",
    "x = np.arange(1, dimensions + 1)\n",
    "y = np.linspace(-1, 1, num=5)\n",
    "margin = 0.5\n",
    "\n",
    "for i, (ax, key) in enumerate(zip(axs, sorted(results))):    \n",
    "    # Show reference lines\n",
    "    for value in np.linspace(-1, 1, num=9):\n",
    "        ax.axhline(value, color=neutral_color3, linewidth=1, linestyle=':', zorder=-1)\n",
    "    ax.axhline(0, color=neutral_color2, linewidth=1)\n",
    "\n",
    "    # Add up all corrections to the score\n",
    "    scores_mean, scores_std = results[key]\n",
    "    scores_zero = (scores_mean['total_score'][0], scores_std['total_score'][0])\n",
    "    \n",
    "    scores_mean = [(adjusted_score, score, np.sum(score0))\n",
    "                   for dimension, adjusted_score, score, *score0 in scores_mean[1:]]\n",
    "    adjusted_score, score, score0 = [np.array(values) for values in zip(*scores_mean)]\n",
    "    \n",
    "    scores_std = [(adjusted_score, score, np.sum(score0))\n",
    "                  for dimension, adjusted_score, score, *score0 in scores_std[1:]]\n",
    "    adjusted_score_std, score_std, score0_std = [np.array(values) for values in zip(*scores_std)]\n",
    "    \n",
    "    # Plot special point\n",
    "    ax.errorbar([1, 2], [scores_zero[0], adjusted_score[1]],\n",
    "                fmt='x', #yerr=[scores_zero[1], 0]\n",
    "                color=cmap2(0.1), markersize=15, capsize=5, linewidth=5)\n",
    "\n",
    "    # Show trend line\n",
    "    constant = np.ones_like(adjusted_score)    \n",
    "    model = np.column_stack((constant, constant))\n",
    "    \n",
    "    corrected_score = adjusted_score.copy()\n",
    "    corrected_score[0] = scores_zero[0]\n",
    "\n",
    "    coeff = np.linalg.lstsq(model, corrected_score, rcond=-1)[0]\n",
    "    ax.axhline(np.sum(coeff), color=cmap2(0.1), linewidth=2, linestyle=':',\n",
    "               label='constant line')\n",
    "\n",
    "    # Plot score contributions\n",
    "    ax.bar(x, score, color=cmap1(0.9), width=0.6,\n",
    "           label=r'$\\langle \\hat{\\mathcal{D}}_{TCMI}(Y; X) \\rangle$', alpha=0.9)\n",
    "    errors = ax.errorbar(x, score, yerr=score_std, color=neutral_color2, clip_on=False,\n",
    "                         fmt='D', markersize=6, capsize=3, linewidth=1, linestyle=None)\n",
    "    for b in errors[1]:\n",
    "        b.set_clip_on(False)\n",
    "    \n",
    "    ax.bar(x, -score0, bottom=0, color=cmap1(0.1), width=0.6,\n",
    "           label=r'$\\langle \\hat{\\mathcal{D}}_{TCMI, 0}(Y; X) \\rangle$', alpha=0.9)\n",
    "    ax.errorbar(x, -score0, yerr=score0_std, color=neutral_color2, clip_on=False,\n",
    "                fmt='D', markersize=6, capsize=3, linewidth=1, linestyle=None)\n",
    "    \n",
    "    ax.errorbar(x, adjusted_score, yerr=adjusted_score_std, color=cmap2(0.9),\n",
    "                fmt='o', label=r'$\\langle \\hat{\\mathcal{D}}_{TCMI}^*(Y; X) \\rangle$', markersize=8,\n",
    "                capsize=5, linewidth=2)\n",
    "\n",
    "    ax.text(x[0] - margin, y[0], 'Sample size: {:d}'.format(key), ha='left',\n",
    "            va='bottom', fontweight='bold', color=neutral_color1, fontsize='small')\n",
    "    \n",
    "    # Plot styles\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x)\n",
    "    ax.set_xlim(x[0] - margin, x[-1] + margin)\n",
    "    #ax.set_xlabel('Subset dimension $|X|$')\n",
    "    \n",
    "    # Show axis only for first subfigure\n",
    "    if i > 0:\n",
    "        ax.set_yticklabels([])\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.set_yticks([])\n",
    "    else:\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels(['{:.1f}'.format(v) for v in np.abs(y)])\n",
    "        ax.set_ylabel('TCMI score contributions\\n'\n",
    "                      r'$\\leftarrow \\langle \\hat{\\mathcal{D}}_0(Y; X) \\rangle$ | '\n",
    "                      r'$\\langle \\hat{\\mathcal{D}}(Y; X) \\rangle \\rightarrow$')\n",
    "    ax.set_ylim(y[0], y[-1])\n",
    "    \n",
    "    # Set equal aspect ratio for all figures\n",
    "    ax.set_aspect(1.5)\n",
    "    \n",
    "    # Show legend\n",
    "    if ax is axs[-1]:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        handles.append(handles.pop(0))\n",
    "        labels.append(labels.pop(0))\n",
    "        cax = fig.add_axes([0.91, 1, .5, 1])\n",
    "        \n",
    "        # HACK: Create legend in different context\n",
    "        legend = plt.legend(handles, labels, loc='lower right', facecolor='w',\n",
    "                            ncol=4, handletextpad=0.5, handlelength=2,\n",
    "                            columnspacing=0.6, bbox_to_anchor=(0.18, -1.05), frameon=False)\n",
    "        cax.remove()\n",
    "        fig.add_artist(legend)\n",
    "        \n",
    "        ax.text(0.33, 0.03, 'Legend:', transform=fig.transFigure)\n",
    "        ax.text(0.07, 0.05, r'Subset dimension $|X| \\longrightarrow$', transform=fig.transFigure)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.4. Invariance against scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Test**: Invariance of TCMI against invertible transformations<br />\n",
    "**Expected**: Same score (here: showcases some very simple examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:44.122288Z",
     "start_time": "2020-02-06T20:01:38.339997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "short_size = 50\n",
    "xx = 2 * np.random.random_sample(short_size) - 1\n",
    "yy = np.linspace(0, 1, num=short_size)\n",
    "\n",
    "target = 'y'\n",
    "data = pd.DataFrame({'y': yy, 'x1': xx, 'x2': np.exp(xx), 'x3': xx**3 + xx})\n",
    "\n",
    "estimator = DependenceEstimator(method='tcmi', n_jobs=-1)\n",
    "get_subspaces(data, target, estimator, cv=None, verbose=1, depth=1,\n",
    "              scoring='mutual_information_score', n_jobs=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Bivariate Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, we examine a simple feature selection task with a known distribution and nonlinear dependencies between features and the output variable. Essentially, we consider bivariate Gaussian distributions with different sample sizes, add noisy features, and test dependency estimators to find the optimal subset of features. Since the ground truth is known and the problem is two-dimensional, we expect only two traits to be selected by all dependency estimators.\n",
    "\n",
    "**Settings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:44.194903Z",
     "start_time": "2020-02-06T20:01:44.138636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "methods = ['tcmi', 'cmi', 'mac', 'uds', 'mcde']\n",
    "sizes = [50, 100, 200, 500]\n",
    "\n",
    "seed = 2019\n",
    "num_splits = 10\n",
    "num_repeats = 500\n",
    "\n",
    "# Test case\n",
    "noise_levels = 5\n",
    "num_samples = 500\n",
    "confidence = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:44.310396Z",
     "start_time": "2020-02-06T20:01:44.203485Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = 'Gaussian'\n",
    "data = pd.read_csv('data/tcmi/2d_gaussian.csv', low_memory=False)\n",
    "#data = utils.prepare_data(data, target)\n",
    "\n",
    "# Get data\n",
    "x, z = data.drop(labels=target, axis=1), data[target]\n",
    "\n",
    "# Setup noise levels\n",
    "noises = np.linspace(0, 1, num=noise_levels + 1)\n",
    "\n",
    "# Generate indices for sub-sampling\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "indices = np.arange(data.shape[0])\n",
    "rng.shuffle(indices)\n",
    "\n",
    "# Plot empirical mean and covariance\n",
    "xy = np.vstack((data['x'], data['y']))\n",
    "pretty_print = lambda x: re.sub('\\s+', ' ', repr(x))\n",
    "print('Empirical mean       = {}\\nEmpirical covariance = {}'\n",
    "      .format(pretty_print(np.mean(xy, axis=-1)), pretty_print(np.cov(xy))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.1. Check score of optimal feature subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Test**: List TCMI scores for optimal two-dimensional subset `{x,y}`<br />\n",
    "**Expected**: TCMI scores of `{x,y}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:44.380950Z",
     "start_time": "2020-02-06T20:01:44.320839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator = DependenceEstimator(method='tcmi', n_jobs=-1)\n",
    "key = 'gaussian_optimal_subspace'\n",
    "\n",
    "gaussian = data.iloc[indices[:sizes[-1]]]\n",
    "\n",
    "scores = get_storage_data(key, None)\n",
    "if scores is None:\n",
    "    scores = []\n",
    "    for k1, k2 in itertools.product(['x', '-x', '|x|', '-|x|'], ['y', '-y', '|y|', '-|y|']):\n",
    "        temp_xy, temp_z = gaussian[[k1, k2]], gaussian[target]\n",
    "        score = estimator.score(temp_xy, temp_z)\n",
    "        scores.append((k1, k2, score))\n",
    "        \n",
    "    scores.sort(key=lambda x: -x[-1])\n",
    "\n",
    "print('\\n'.join('{{{:s},{:s}}} = {:.3f}'.format(*s) for s in scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.2. Compute subset score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Test**: Compute subset score `{x, y}` -> z with respect to increasing number of data points<br />\n",
    "**Expected**: Monotonic increase for information-theoretic measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:44.492316Z",
     "start_time": "2020-02-06T20:01:44.415300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for size in sizes:\n",
    "    gaussian = np.take(xy, indices[:size], axis=-1)\n",
    "    output = np.take(z, indices[:size], axis=0)\n",
    "    gaussian = gaussian.T\n",
    "    \n",
    "    results[size] = {}\n",
    "    for method in methods:\n",
    "        estimator = DependenceEstimator(method=method, n_jobs=-1)\n",
    "        key = 'gaussian_xy_{:s}_{:d}'.format(method, size)\n",
    "        \n",
    "        score = get_storage_data(key, default=None, overwrite=bool(method != 'tcmi'))\n",
    "        if score is None:\n",
    "            score = estimator.score(gaussian, output)        \n",
    "        results[size].setdefault(method, score)\n",
    "    \n",
    "df = np.array([[results[size][method] for method in methods] for size in sizes])\n",
    "pd.DataFrame(df, index=sizes, columns=methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.3. Find optimal subset of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Test**: Find optimal subset of features<br />\n",
    "**Expected**: Optimal subset must be `{x, y}`, prefactors allowed, feature \"normal\" is similar to \"x\" or \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:44.598306Z",
     "start_time": "2020-02-06T20:01:44.501472Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for size in sizes:\n",
    "    gaussian = data.iloc[indices[:size]]\n",
    "    \n",
    "    print('\\n/**'\n",
    "          '\\n * Data points = {:d}'\n",
    "          '\\n */'.format(size))\n",
    "    \n",
    "    results[size] = {}\n",
    "    for method in methods:\n",
    "        estimator = DependenceEstimator(method=method, n_jobs=-1)\n",
    "        key = 'gaussian_subspace_{:s}_{:d}'.format(method, size)\n",
    "        \n",
    "        subsets = get_storage_data(key, default=None, overwrite=bool(method != 'tcmi'))\n",
    "        if subsets is None:\n",
    "            subsets = tcmi.get_subspaces(gaussian, target, estimator, cv=None,\n",
    "                                         depth=2, scoring='mutual_information_score',\n",
    "                                         fit_params=None, verbose=1, n_jobs=-1)\n",
    "            \n",
    "        subsets = utils.filter_subsets(subsets, remove_duplicates=True)\n",
    "        \n",
    "        output = []\n",
    "        cursor = 3\n",
    "        cursor = -1\n",
    "        threshold = 1\n",
    "        \n",
    "        has_xy = False\n",
    "        for i, subset in enumerate(subsets):\n",
    "            subspace = subset['subspace']\n",
    "            score = subset['stats']['mutual_information_score_mean']\n",
    "            depth = len(subspace)\n",
    "            \n",
    "            if i == 0 or cursor == -1 or cursor > depth:\n",
    "                line = '[{:>3d}] {{{:s}}} = {:.2f}'.format(\n",
    "                    len(subspace), ','.join(subspace), score)\n",
    "                output.append(line)\n",
    "                \n",
    "                if i > 0 or cursor == -1:\n",
    "                    threshold = 0.95 * score\n",
    "                    cursor = depth\n",
    "            \n",
    "            elif score > threshold:\n",
    "                line = '[{:>3d}] {{{:s}}} = {:.2f}'.format(\n",
    "                    len(subspace), ','.join(subspace), score)\n",
    "                output.append(line)\n",
    "        \n",
    "        results[size].setdefault(method, subsets)\n",
    "        print('\\nMethod: {:s}\\n  {:s}'.format(method, '\\n  '.join(output)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.4. Statistical power analysis (95% confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To perform statistical power analysis we see, that all of the above dependency measures converge to the optimal feature subsets ${x,y}$ for at least 500 data samples, which will be chosen as the sample size in the following.\n",
    "\n",
    "**Test**: Statistical power analysis (95% confidence)<br />\n",
    "**Expected**: High statistical power as well as high contrast between the actual score and independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:44.841663Z",
     "start_time": "2020-02-06T20:01:44.602723Z"
    },
    "code_folding": [
     0,
     34
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset(data, target, noise, n_splits=10, n_repeats=100, seed=None):\n",
    "    \"\"\"Generate data set.\n",
    "    \"\"\"\n",
    "    size = data.shape[0]\n",
    "\n",
    "    # Convert cross-validation into number of data samples\n",
    "    cv = np.floor(size * (n_splits if n_splits < 1 else (1 - 1 / n_splits))).astype(np.int_)\n",
    "\n",
    "    # Initialize generator\n",
    "    rng = np.random.RandomState(seed)\n",
    "    indices = np.arange(size)\n",
    "\n",
    "    # Evaluate dataset\n",
    "    noise /= 2\n",
    "    for _ in range(n_repeats):\n",
    "        # Split dataset and make sure to make vectors immutable\n",
    "        index = indices[:cv].copy()\n",
    "        dataset = data.iloc[index].copy()\n",
    "        \n",
    "        # Add (centered) noise\n",
    "        for key in dataset:\n",
    "            if key == target:\n",
    "                continue\n",
    "            \n",
    "            # Center noise around actual value\n",
    "            dataset[key] += rng.uniform(-noise, noise, size=cv)\n",
    "\n",
    "        # Return generated dataset\n",
    "        yield dataset, index\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        rng.shuffle(indices)\n",
    "        \n",
    "        \n",
    "def compute_score(method, noise, dataset, indices):\n",
    "    \"\"\"Compute score for dataset.\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    test_xy, test_z = dataset.drop(labels=target, axis=1), dataset[target]\n",
    "    estimator = DependenceEstimator(method=method, n_jobs=1, cache=None)\n",
    "    h = cache.compute_hash((dataset, indices))\n",
    "\n",
    "    # Compute measure with respect to output\n",
    "    key = 'score_{:s}_{:.2f}_{:s}'.format(method, noise, h)\n",
    "    score = storage.get(key, None)\n",
    "    if score is None:\n",
    "        score = estimator.score(test_xy, test_z)\n",
    "        storage.set(key, score, retry=True)\n",
    "\n",
    "    # Compute measure with respect to independent variables\n",
    "    key = 'score0_{:s}_{:.2f}_{:s}'.format(method, noise, h)\n",
    "    score0 = storage.get(key, None)\n",
    "    if score0 is None:\n",
    "        score0 = estimator.score(test_independent[indices], test_z)\n",
    "        storage.set(key, score0, retry=True)\n",
    "\n",
    "    # Return scores\n",
    "    return score, score0\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    ('Bivariate normal distribution: $\\{x,y\\}$', 'gaussian', 'data/tcmi/2d_gaussian.csv', \n",
    "     'Gaussian', ('-|x|', '-|y|'))\n",
    "]\n",
    "\n",
    "collections = []\n",
    "for label, name, file, target, subset in datasets:\n",
    "    data = pd.read_csv(file, low_memory=False)\n",
    "    if target == 'Delta E':\n",
    "        del data['Combination']\n",
    "        \n",
    "    print('\\n/**'\n",
    "          '\\n * Data set: {:s}'\n",
    "          '\\n */\\n'.format(name))\n",
    "        \n",
    "    data = utils.prepare_data(data, target)\n",
    "    size = min(len(data), num_samples)\n",
    "    subset += (target, )\n",
    "\n",
    "    # Setup noise levels\n",
    "    noises = np.linspace(0, 1, num=noise_levels + 1)\n",
    "\n",
    "    # Generate indices for test set\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    indices = np.arange(len(data))\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # Consider subset of data samples\n",
    "    data = data[list(subset)].iloc[indices[:size]]\n",
    "    test_independent = rng.uniform(0, 1, (size, 2))\n",
    "\n",
    "    dtypes = [('noise', np.float_), ('power', np.float_),\n",
    "              ('score', np.float_), ('score0', np.float_)]\n",
    "    print('Data points = {:d}\\nNoise levels = {:d}'\n",
    "          .format(size, len(noises)))\n",
    "\n",
    "    # Initialize joblib\n",
    "    processor = joblib.Parallel(n_jobs=-1)\n",
    "    callback = joblib.delayed(compute_score)\n",
    "    \n",
    "    # Compute statistical power\n",
    "    collection = {}\n",
    "    for method in methods:\n",
    "        print('\\nMethod: {:s}\\n  '.format(method), end='')\n",
    "\n",
    "        statistics = np.zeros(noises.size, dtype=dtypes)\n",
    "        for i, noise in enumerate(noises):\n",
    "            print('{:.2f}'.format(noise), end=', ')\n",
    "\n",
    "            key = '{:s}_statistical_power_{:s}_{:d}_noise_{:.2f}'\n",
    "            key = key.format(name, method, size, noise)\n",
    "            values = get_storage_data(key, default=None, overwrite=bool(method != 'tcmi'))\n",
    "\n",
    "            if values is None:\n",
    "                # Perform calculation in parallel\n",
    "                iterator = generate_dataset(data, target, noise, n_splits=num_splits,\n",
    "                                            n_repeats=num_repeats, seed=seed)\n",
    "                values = processor(callback(name, method, noise, dataset, indices)\n",
    "                                   for dataset, indices in iterator)\n",
    "                scores, scores0 = tuple(zip(*values))\n",
    "\n",
    "                values = tuple(np.array(value) for value in zip(*values))\n",
    "\n",
    "            # Compute statistical power\n",
    "            scores, scores0 = values\n",
    "            cutoff = np.quantile(scores0, confidence)\n",
    "\n",
    "            power = np.mean(scores > cutoff)\n",
    "            score_avg = scores.mean()\n",
    "            score0_avg = scores0.mean()\n",
    "\n",
    "            # Save statistical power statistics\n",
    "            statistics[i] = (noise, power, score_avg, score0_avg)\n",
    "\n",
    "        print('\\n')\n",
    "        for values in statistics:\n",
    "            print('Noise = {:.2f}, Power = {:.2f}, Score = {:.2f}, Score0 = {:.2f}'.format(*values))     \n",
    "        collection[method] = statistics\n",
    "    \n",
    "    collections.append((label, collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:45.959653Z",
     "start_time": "2020-02-06T20:01:44.869348Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Plot\n",
    "##\n",
    "\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "# Plot\n",
    "margin = 0.1\n",
    "ncols = len(methods)\n",
    "nrows = len(collections)\n",
    "idx = np.where(np.arange(noise_levels + 1) % 2 == 0)[0]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(16, 3 * nrows), ncols=ncols, nrows=nrows) \n",
    "if len(collections) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "handlers = []\n",
    "for i, (caxs, item) in enumerate(zip(axs, collections)):\n",
    "    name, collection = item\n",
    "    \n",
    "    for j, (ax, method) in enumerate(zip(caxs, methods)):\n",
    "        # Show reference lines\n",
    "        for y in np.linspace(0, 1, num=5):\n",
    "            ax.axhline(y, color=neutral_color3, linewidth=1, linestyle=':', zorder=-1)\n",
    "\n",
    "        # Get data\n",
    "        data = collection[method]\n",
    "\n",
    "        # Initialize coordinates\n",
    "        x = data['noise']\n",
    "        y = np.linspace(0, 1, num=5)\n",
    "        color = 0.8 * (j / len(methods)) + 0.1\n",
    "\n",
    "        # Show contrast of score\n",
    "        score = np.maximum(data['score'] - data['score0'], 1e-2)\n",
    "        bars = ax.bar(x, data['score'], data['score0'])\n",
    "        handler = ax.fill_between(x, data['score'], data['score0'], color=cmap1(color),\n",
    "                                  label='Dependence scores')\n",
    "        handlers.append(handler)\n",
    "        \n",
    "        if j == 0:\n",
    "            ax.text(x.min() - 0.1, y.max() + 0.17, name, va='bottom', ha='left')\n",
    "\n",
    "        ax.plot(x, data['power'], color=cmap2(0.9), marker='s', clip_on=False,\n",
    "                label='Statistical power ($\\gamma = 0.95$)')\n",
    "\n",
    "        ax.text(x[0] - margin, y[-1] + 0.05, method.upper(), ha='left',\n",
    "                va='bottom', fontweight='bold', color=neutral_color1, fontsize='small')\n",
    "\n",
    "        # Show statistical power\n",
    "        for bar, value, value0, score0 in zip(bars, data['power'], score, data['score0']):\n",
    "            bar_x = bar.get_x() + bar.get_width() / 2 # - 0.01 * bar.get_width()\n",
    "            bar_y = bar.get_y() + bar.get_height() + 0.05\n",
    "            text = '{:.2f}'.format(value)\n",
    "\n",
    "            va = 'bottom'\n",
    "            if value > 0.5:\n",
    "                value -= 0.04\n",
    "                va = 'top'\n",
    "\n",
    "            if value < 0.5: # or value > 0.9:\n",
    "                value += 0.08\n",
    "                va = 'bottom'\n",
    "                \n",
    "            text = ax.text(bar_x, value - 0.02, text, ha='center', va=va,\n",
    "                           fontsize='small', color=cmap2(0.9), rotation=90)\n",
    "            text.set_path_effects(\n",
    "                [path_effects.withStroke(linewidth=5, foreground='w')])\n",
    "\n",
    "        bars.remove()\n",
    "\n",
    "        # Plot styles\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        ax.spines['left'].set_position(('outward', 10))\n",
    "        ax.spines['bottom'].set_position(('outward', 10))\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xlim(x[0] - margin, x[-1] + margin)\n",
    "        #ax.set_xlabel('Noise levels')\n",
    "\n",
    "        # Show axis only for first subfigure\n",
    "        if j > 0:\n",
    "            ax.set_yticklabels([])\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.set_yticks(y)\n",
    "            ax.set_yticklabels(['{:.2f}'.format(value) for value in y])\n",
    "            ax.set_ylabel('Score/Power')\n",
    "        ax.set_ylim(y[0], y[-1])\n",
    "        ax.set_xlim(x[0], x[-1])\n",
    "    \n",
    "    # Show legend\n",
    "    if caxs is axs[-1]:\n",
    "        handles, labels = ax.get_legend_handles_labels() \n",
    "        handles[-1] = tuple(handlers)\n",
    "        \n",
    "        cax = fig.add_axes([1, 1, 0, 1])\n",
    "        \n",
    "        # HACK: Create legend in different context\n",
    "        # https://matplotlib.org/3.1.1/gallery/text_labels_and_annotations/legend_demo.html\n",
    "        legend = plt.legend(handles[::-1], labels[::-1], loc='upper right', facecolor='w',\n",
    "                            ncol=3, handletextpad=0.3, handlelength=2, numpoints=1,\n",
    "                            columnspacing=0.75, bbox_to_anchor=(0, -1), frameon=False,\n",
    "                            handler_map={tuple: HandlerTuple(ndivide=None,pad=0)})\n",
    "        \n",
    "        # Get the bounding box of the original legend\n",
    "        bb = legend.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "        # Change to location of the legend. \n",
    "        xOffset = -0.6\n",
    "        bb.x0 += xOffset\n",
    "        bb.x1 += xOffset\n",
    "        legend.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "        cax.remove()\n",
    "        fig.add_artist(legend)\n",
    "        \n",
    "        ax.text(0.48, -0.12, 'Legend:', transform=fig.transFigure)\n",
    "        ax.text(0.07, -0.12, r'Noise levels $\\longrightarrow$', transform=fig.transFigure)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. UCI Regression datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Friedman - https://sci2s.ugr.es/keel/dataset.php?cod=81 . It has been obtained from the LIACC repository. The original page where the data set can be found is: http://www.liaad.up.pt/~ltorgo/Regression/DataSets.html.\n",
    "2. Concrete Compressive Strength - https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
    "3. Forest Fires Data Set - https://archive.ics.uci.edu/ml/datasets/Forest+Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:46.090055Z",
     "start_time": "2020-02-06T20:01:45.980293Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "methods = ['tcmi', 'cmi', 'mac', 'uds', 'mcde']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3.1. Generate Friedman dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:46.302638Z",
     "start_time": "2020-02-06T20:01:46.105784Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/\n",
    "# Friedman #1 regression dataset \n",
    "\n",
    "np.random.seed(0)\n",
    " \n",
    "size = 500\n",
    "target = 'y'\n",
    "\n",
    "# \"Friedamn #1‚Äù regression problem\n",
    "X = np.random.uniform(0, 1, (size, 14))\n",
    "Y = (10 * np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2] - .5)**2 +\n",
    "     10*X[:,3] + 5*X[:,4] + np.random.normal(0,1))\n",
    "#Add 3 additional correlated variables (correlated with X1-X3)\n",
    "X[:,10:] = X[:,:4] + np.random.normal(0, .025, (size,4))\n",
    " \n",
    "names = [\"x%s\" % (i + 1) for i in range(X.shape[-1])]\n",
    "data = pd.DataFrame(data=X, columns=names)\n",
    "data[target] = Y\n",
    "\n",
    "# data.to_csv('data/friedman.csv', index=False)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Show which features have strong correlations\n",
    "for key in data:\n",
    "    if key == target:\n",
    "        continue\n",
    "    \n",
    "    r2 = np.corrcoef(data[key], data[target])[0, 1]**2\n",
    "    rho, pval = stats.spearmanr(data[key], data[target])\n",
    "    print('{:<3s}: r2={:<4.2f}  rho={:>5.2f}  pval={:<.2g}'\n",
    "          .format(key, r2, rho, pval))\n",
    "    \n",
    "x, z = data.drop(labels=target, axis=1), data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3.2. Optimal subspace search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Test**: Find optimal subsets for several available datasets<br />\n",
    "**Expected**: Convergence and stability of all dependence measures to the same subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:46.620148Z",
     "start_time": "2020-02-06T20:01:46.315591Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets = [\n",
    "    ('friedman', 'data/tcmi/friedman.csv', 'y'),\n",
    "    ('concrete', 'data/tcmi/concrete.csv', 'compressive_strength'),\n",
    "    ('forest_fires', 'data/tcmi/forestfires.csv', 'area')\n",
    "]\n",
    "\n",
    "for name, filename, target in datasets:\n",
    "    data = pd.read_csv(filename, low_memory=False)\n",
    "    data = utils.prepare_data(data, target)\n",
    "    \n",
    "    print('\\n/**'\n",
    "          '\\n * Model: {:s}'\n",
    "          '\\n */\\n'\n",
    "          '\\nKeys: {{{:s}}}'\n",
    "          '\\nSize: {:d}'\n",
    "          .format(name, ','.join(data), data.shape[-1] - 1))\n",
    "    \n",
    "    for method in methods:\n",
    "        estimator = DependenceEstimator(method=method, n_jobs=1)\n",
    "        key = 'subspace_search_{:s}_{:s}'.format(target, method)\n",
    "        print('\\nMethod: {:s}'.format(method))\n",
    "\n",
    "        subsets = get_storage_data(key, default=None, overwrite=bool(method != 'tcmi'))\n",
    "        if subsets is None:\n",
    "            subsets = tcmi.get_subspaces(data, target, estimator, cv=None,\n",
    "                                         depth=-1, scoring='mutual_information_score',\n",
    "                                         fit_params=None, verbose=1, n_jobs=-1)\n",
    "\n",
    "        output = []\n",
    "        if len(subsets) > 1000:\n",
    "            output.append('More than {:d} subsets.'.format(len(subsets)))\n",
    "        else:            \n",
    "            cursor = -1\n",
    "            threshold = 1\n",
    "            for i, subset in enumerate(subsets):\n",
    "                subspace = subset['subspace']\n",
    "                score = subset['stats']['mutual_information_score_mean']\n",
    "                depth = len(subspace)\n",
    "\n",
    "                if i == 0 or cursor == -1 or cursor > depth:\n",
    "                    line = '[{:>3d}] {{{:s}}} = {:.2f}'.format(\n",
    "                        len(subspace), ','.join(subspace), score)\n",
    "                    output.append(line)\n",
    "\n",
    "                    if i > 0 or cursor == -1:\n",
    "                        threshold = 0.95 * score\n",
    "                        cursor = depth\n",
    "\n",
    "                elif score > threshold:\n",
    "                    line = '[{:>3d}] {{{:s}}} = {:.2f}'.format(\n",
    "                        len(subspace), ','.join(subspace), score)\n",
    "                    output.append(line)\n",
    "        print('  {:s}'.format('\\n  '.join(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Octet-binary compound semiconductors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Octet-binary compound semiconductors are materials consisting of two elements formed by groups of I/VII, II/VI, III/V, or IV/IV elements leading to a full valence shell. They crystallize in rock salt (RS) or zinc blende (ZB) structures.\n",
    "\n",
    "The data set is composed of 82 materials with two atomic species in the unit cell. The objective is to accurately predict the energy difference $\\Delta E$ between RS and ZB structures based on 8 electro-chemical atomic properties for each atomic species $A/B$ (in total 16) such as atomic ionization potential $\\text{IP}$, electron affinity $\\text{EA}$, the energies of the highest-occupied and lowest-unoccupied Kohn-Sham levels, $\\text{H}$ and $\\text{L}$, and the expectation value of the radial probability densities of the valence $s$-, $p$-, and $d$-orbitals, $r_s$, $r_p$, and $r_d$, respectively.\n",
    "\n",
    "<div style=\"padding: 1ex; margin-top: 1ex; margin-bottom: 1ex; border-style: dotted; border-width: 1pt; border-color: blue; border-radius: 3px;\">\n",
    "    L. M. Ghiringhelli, J. Vybiral, S. V. Levchenko, C. Draxl, C. & M. Scheffler: Big Data of Materials Science: Critical Role of the Descriptor. Physical Review Letters <strong>114</strong>, 105503 (2015). DOI: <a href=\"https://dx.doi.org/10.1103/PhysRevLett.114.105503\">10.1103/PhysRevLett.114.105503</a>\n",
    "</div>\n",
    "\n",
    "The additional features from the reference are:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    D1 = \\frac{\\text{IP}(B) - \\text{EA}(B)}{r_p(A)^2}\\ ,\\quad \\ D2 = \\frac{|r_s(A) - r_p(B)|}{\\exp[r_s(A)]} \\ .\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T11:32:15.336979Z",
     "start_time": "2020-01-14T11:32:15.333046Z"
    }
   },
   "source": [
    "**Test**: Optimal feature subset search<br />\n",
    "**Expected**: Find features to best predict $\\Delta E$. Potential candidates are listed in the reference above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T20:01:46.843772Z",
     "start_time": "2020-02-06T20:01:46.664345Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "methods = ['tcmi', 'cmi', 'mac', 'uds', 'mcde']\n",
    "data = pd.read_csv('data/tcmi/octet-binary-compound-semiconductors.csv', low_memory=False)\n",
    "materials = data.drop(columns='Combination', inplace=True)\n",
    "target = 'Delta E'\n",
    "\n",
    "# Add extra features from PRL\n",
    "extra = {\n",
    "    'D1': (data['EA(B)'] - data['IP(B)']) / data['rp(A)']**2,\n",
    "    'D2': np.abs(data['rs(A)'] - data['rp(B)']) / np.exp(data['rs(A)'])\n",
    "}\n",
    "# for k, v in extra.items():\n",
    "#     data[k] = v\n",
    "\n",
    "data = utils.prepare_data(data, target, copy=True)\n",
    "\n",
    "print('\\n/**'\n",
    "      '\\n * Model: Octet-binary compound semiconductor'\n",
    "      '\\n */\\n'\n",
    "      '\\nKeys: {{{:s}}}'\n",
    "      '\\nSize: {:d}'\n",
    "      .format(','.join(data), data.shape[-1] - 1))\n",
    "        \n",
    "method = 'tcmi'\n",
    "estimator = DependenceEstimator(method=method)\n",
    "    \n",
    "key = 'binary_octets_{:s}'.format(method)\n",
    "print('\\nMethod: {:s}'.format(method))\n",
    "\n",
    "subsets = get_storage_data(key, None)\n",
    "if subsets is None: \n",
    "    subsets = tcmi.get_subspaces(data, target, estimator, cv=None,\n",
    "                                 depth=-1, scoring='mutual_information_score',\n",
    "                                 fit_params=None, verbose=1, n_jobs=-1)\n",
    "\n",
    "threshold = 1\n",
    "output = []\n",
    "cursor = -1\n",
    "subsets = utils.filter_subsets(subsets)\n",
    "\n",
    "candidates = []\n",
    "for i, subset in enumerate(subsets):\n",
    "    subspace = subset['subspace']\n",
    "    score = subset['stats']['mutual_information_score_mean']\n",
    "    depth = len(subspace)\n",
    "\n",
    "    if i == 0 or cursor == -1 or cursor > depth:\n",
    "        line = '[{:>3d}] {{{:s}}} = {:.2f}'.format(\n",
    "            len(subspace), ','.join(subspace), score)\n",
    "        candidates.append((subspace, score))\n",
    "        output.append(line)\n",
    "\n",
    "        if i > 0 or cursor == -1:\n",
    "            threshold = 0.95 * score\n",
    "            cursor = depth\n",
    "\n",
    "    elif score > threshold:\n",
    "        line = '[{:>3d}] {{{:s}}} = {:.2f}'.format(\n",
    "            len(subspace), ','.join(subspace), score)\n",
    "        candidates.append((subspace, score))\n",
    "        output.append(line)\n",
    "\n",
    "print('  {:s}'.format('\\n  '.join(output)))\n",
    "keys = sorted([k for k in data.columns if k != target and '|' not in k and '-' not in k],\n",
    "              key=lambda x: (x[-2], x))\n",
    "candidates.insert(0, (keys, 1))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
